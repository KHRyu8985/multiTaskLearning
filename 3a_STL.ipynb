{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a7b76725",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import fastmri\n",
    "from fastmri.data import transforms\n",
    "from fastmri.models.unet import Unet\n",
    "from fastmri.models.varnet import *\n",
    "\n",
    "import sigpy as sp\n",
    "from sigpy import from_pytorch\n",
    "import sigpy.plot as pl\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (10,10)\n",
    "import skimage.metrics\n",
    "\n",
    "from dloader import genDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed139960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # command line argument parser\n",
    "# parser = argparse.ArgumentParser(\n",
    "#     description = 'define parameters and roots for STL training'\n",
    "# )\n",
    "\n",
    "# # hyperparameters\n",
    "# parser.add_argument(\n",
    "#     '--epochs', default=2, type=int,\n",
    "#     help='number of epochs to run'\n",
    "# )\n",
    "# parser.add_argument(\n",
    "#     '--lr', default=0.0002, type=float,\n",
    "#     help='learning rate'\n",
    "# )\n",
    "\n",
    "\n",
    "# # model training\n",
    "# parser.add_argument(\n",
    "#     '--numblocks', default=12, type=int,\n",
    "#     help='number of unrolled blocks in total'\n",
    "# )\n",
    "# parser.add_argument(\n",
    "#     '--network', default='varnet',\n",
    "#     help='type of network ie unet or varnet'\n",
    "# )\n",
    "# parser.add_argument(\n",
    "#     '--device', default='cuda:2',\n",
    "#     help='cuda:2 device default'\n",
    "# )\n",
    "\n",
    "\n",
    "# # dataset properties\n",
    "# parser.add_argument(\n",
    "#     '--datasets', nargs='+',\n",
    "#     help='names of one or two sets of data files i.e. div_coronal_pd',\n",
    "#     required = True\n",
    "# )\n",
    "# parser.add_argument(\n",
    "#     '--scarcities', default=[3, 4], type=int, nargs='+',\n",
    "#     help='number of samples in second contrast will be decreased by 1/2^N'\n",
    "#     )\n",
    "# parser.add_argument(\n",
    "#     '--undersamples', default=[6], type=int, nargs='+',\n",
    "#     help='undersampling factor of k-space'\n",
    "#     )\n",
    "# parser.add_argument(\n",
    "#     '--centerfracs', default=[0.06], type=int, nargs='+',\n",
    "#     help='center fractions sampled of k-space'\n",
    "#     )\n",
    "\n",
    "\n",
    "# # paths\n",
    "# parser.add_argument(\n",
    "#     '--modelpath', default='models',\n",
    "#     help='path to save best model'\n",
    "# )\n",
    "\n",
    "\n",
    "# # save / display data\n",
    "# parser.add_argument(\n",
    "#     '--experimentname', default='unnamed_experiment',\n",
    "#     help='experiment name i.e. STL_unet or MTAN_pareto_varnet etc.'\n",
    "# )\n",
    "# parser.add_argument(\n",
    "#     '--verbose', default=True, type=bool,\n",
    "#     help='''if true, prints to console and creatues full TensorBoard\n",
    "#     (if tensorboard is also True)'''\n",
    "# )\n",
    "# parser.add_argument(\n",
    "#     '--tensorboard', default=True, type=bool,\n",
    "#     help='if true, creates TensorBoard'\n",
    "# )\n",
    "# parser.add_argument(\n",
    "#     '--savefreq', default=15, type=int,\n",
    "#     help='how many epochs per saved recon image'\n",
    "# )\n",
    "\n",
    "# opt = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffef36ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can make one iteration block like this\n",
    "class VarNetBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    This model applies a combination of soft data consistency with the input\n",
    "    model as a regularizer. A series of these blocks can be stacked to form\n",
    "    the full variational network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model: nn.Module):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model: Module for \"regularization\" component of variational\n",
    "                network.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = model\n",
    "        self.eta = nn.Parameter(torch.ones(1))\n",
    "\n",
    "    def sens_expand(self, x: torch.Tensor, sens_maps: torch.Tensor) -> torch.Tensor:\n",
    "        return fastmri.fft2c(fastmri.complex_mul(x, sens_maps)) # F*S operator\n",
    "\n",
    "    def sens_reduce(self, x: torch.Tensor, sens_maps: torch.Tensor) -> torch.Tensor:\n",
    "        x = fastmri.ifft2c(x)\n",
    "        return fastmri.complex_mul(x, fastmri.complex_conj(sens_maps)).sum(\n",
    "            dim=1, keepdim=True\n",
    "        ) # S^H * F^H operator\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        current_kspace: torch.Tensor,\n",
    "        ref_kspace: torch.Tensor,\n",
    "        mask: torch.Tensor,\n",
    "        sens_maps: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        mask = mask.bool()\n",
    "        zero = torch.zeros(1, 1, 1, 1, 1).to(current_kspace)\n",
    "        soft_dc = torch.where(mask, current_kspace - ref_kspace, zero) * self.eta\n",
    "        model_term = self.sens_expand(\n",
    "            self.model(self.sens_reduce(current_kspace, sens_maps)), sens_maps\n",
    "        )\n",
    "\n",
    "        return current_kspace - soft_dc - model_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e7d9ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can stack VarNetBlocks to make a unrolled VarNet (with 10 blocks)\n",
    "\n",
    "\n",
    "class VarNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A full variational network model.\n",
    "\n",
    "    This model applies a combination of soft data consistency with a U-Net\n",
    "    regularizer. To use non-U-Net regularizers, use VarNetBock.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_cascades: int = 12,\n",
    "        chans: int = 18,\n",
    "        pools: int = 4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cascades = nn.ModuleList(\n",
    "            [VarNetBlock(NormUnet(chans, pools)) for _ in range(num_cascades)]\n",
    "        )\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        masked_kspace: torch.Tensor, \n",
    "        mask: torch.Tensor,\n",
    "        sens_maps: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \n",
    "        kspace_pred = masked_kspace.clone()\n",
    "\n",
    "        for cascade in self.cascades:\n",
    "            kspace_pred = cascade(kspace_pred, masked_kspace, mask, sens_maps)\n",
    "        \n",
    "        im_coil = fastmri.ifft2c(kspace_pred)\n",
    "        im_comb = fastmri.complex_mul(im_coil, fastmri.complex_conj(sens_maps)).sum(\n",
    "            dim=1, keepdim=True\n",
    "        )\n",
    "        \n",
    "        return kspace_pred, im_comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "486cf77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(\n",
    "        p.numel() for p in model.parameters() if p.requires_grad\n",
    "    )\n",
    "\n",
    "def _test_result(im_fs: torch.Tensor, im_us: torch.Tensor) -> np.ndarray:\n",
    "\n",
    "    with torch.no_grad():\n",
    "        im_us = from_pytorch(im_us.cpu().detach(),iscomplex = True)\n",
    "        im_fs = from_pytorch(im_fs.cpu().detach(), iscomplex = True)\n",
    "        im_us = np.abs(im_us).squeeze()\n",
    "        im_fs = np.abs(im_fs).squeeze()\n",
    "        \n",
    "        im_us = sp.resize(im_us, [360, 320])\n",
    "        im_fs = sp.resize(im_fs, [360, 320])\n",
    "        \n",
    "        out_cat = np.concatenate((im_fs, im_us), 1)\n",
    "        error_cat = np.concatenate((im_fs, im_fs), 1)\n",
    "        error_cat = np.abs(error_cat - out_cat) * 5\n",
    "        \n",
    "        out_cat = np.concatenate((error_cat, out_cat,), axis=0)\n",
    "        out_cat = out_cat * 1.5  \n",
    "        \n",
    "    return np.flip(out_cat)\n",
    "\n",
    "\n",
    "def plot_quadrant(im_fs, im_us):\n",
    "    fig = plt.figure()\n",
    "    plt.imshow(_test_result(im_fs, im_us), cmap = 'gray', vmax = 2.5) # or normalize between 0-1\n",
    "    plt.close(fig)\n",
    "    return fig\n",
    "\n",
    "\n",
    "def _param_dict(lr, epochs, undersampling, scarcities, center_fractions):\n",
    "    params = {}\n",
    "    params['lr'] = lr\n",
    "    params['epochs'] = epochs\n",
    "    \n",
    "    for i in range(len(undersampling)):\n",
    "        params[f'accerlation_{i}'] = undersampling[i]\n",
    "    \n",
    "    for i in range(len(scarcities)):\n",
    "        params[f'scarcity_{i}'] = scarcities[i]\n",
    "        \n",
    "    for i in range(len(center_fractions)):\n",
    "        params[f'center_fraction_{i}'] = center_fractions[i]\n",
    "    return params\n",
    "\n",
    "\n",
    "def write_tensorboard(writer, avg_cost, model, total_epochs, ratio, opt):\n",
    "    if len(avg_cost.keys()) == 2:\n",
    "        write_tensorboard_one_contrasts(\n",
    "            writer, avg_cost, model, total_epochs, ratio, opt\n",
    "        )\n",
    "    else:\n",
    "        write_tensorboard_two_contrasts(\n",
    "            writer, avg_cost, model, total_epochs, ratio, opt\n",
    "        )\n",
    "\n",
    "\n",
    "def write_tensorboard_two_contrasts(writer, avg_cost, model, total_epochs, ratio, opt):\n",
    "    #write to tensorboard ###opt###\n",
    "    contrast_1, contrast_2, _ = avg_cost.keys()\n",
    "    \n",
    "    for epoch in range(0, total_epochs): ###opt###\n",
    "        writer.add_scalars(\n",
    "            f'{ratio}/l1', {\n",
    "                f'train/{contrast_1}' : avg_cost[contrast_1][epoch, 0],\n",
    "                f'val/{contrast_1}' : avg_cost[contrast_1][epoch, 4],\n",
    "                f'train/{contrast_2}' : avg_cost[contrast_2][epoch, 0],\n",
    "                f'val/{contrast_2}' : avg_cost[contrast_2][epoch, 4],\n",
    "            }, \n",
    "            epoch\n",
    "        )\n",
    "\n",
    "        writer.add_scalars(\n",
    "            f'{ratio}/ssim', {\n",
    "                f'train/{contrast_1}' : avg_cost[contrast_1][epoch, 1],\n",
    "                f'val/{contrast_1}' : avg_cost[contrast_1][epoch, 5],\n",
    "                f'train/{contrast_2}' : avg_cost[contrast_2][epoch, 1],\n",
    "                f'val/{contrast_2}' : avg_cost[contrast_2][epoch, 5],\n",
    "            }, \n",
    "            epoch\n",
    "        )\n",
    "\n",
    "        writer.add_scalars(\n",
    "            f'{ratio}/psnr', {\n",
    "                f'train/{contrast_1}' : avg_cost[contrast_1][epoch, 2],\n",
    "                f'val/{contrast_1}' : avg_cost[contrast_1][epoch, 6],\n",
    "                f'train/{contrast_2}' : avg_cost[contrast_2][epoch, 2],\n",
    "                f'val/{contrast_2}' : avg_cost[contrast_2][epoch, 6],\n",
    "            }, \n",
    "            epoch\n",
    "        )\n",
    "\n",
    "        writer.add_scalars(\n",
    "            f'{ratio}/nrmse', {\n",
    "                f'train/{contrast_1}' : avg_cost[contrast_1][epoch, 3],\n",
    "                f'val/{contrast_1}' : avg_cost[contrast_1][epoch, 7],\n",
    "                f'train/{contrast_2}' : avg_cost[contrast_2][epoch, 3],\n",
    "                f'val/{contrast_2}' : avg_cost[contrast_2][epoch, 7],\n",
    "            }, \n",
    "            epoch\n",
    "        )\n",
    "        \n",
    "        writer.add_scalars(\n",
    "            'overall/l1', {\n",
    "                f'val/{ratio}/{contrast_1}' : avg_cost[contrast_1][epoch, 4],\n",
    "                f'val/{ratio}/{contrast_2}' : avg_cost[contrast_2][epoch, 4],\n",
    "            }, \n",
    "            epoch\n",
    "        )\n",
    "\n",
    "        writer.add_scalars(\n",
    "            'overall/ssim', {\n",
    "                f'val/{ratio}/{contrast_1}' : avg_cost[contrast_1][epoch, 5],\n",
    "                f'val/{ratio}/{contrast_2}' : avg_cost[contrast_2][epoch, 5],\n",
    "            }, \n",
    "            epoch\n",
    "        )\n",
    "\n",
    "        writer.add_scalars(\n",
    "            'overall/psnr', {\n",
    "                f'val/{ratio}/{contrast_1}' : avg_cost[contrast_1][epoch, 6],\n",
    "                f'val/{ratio}/{contrast_2}' : avg_cost[contrast_2][epoch, 6],\n",
    "            }, \n",
    "            epoch\n",
    "        )\n",
    "\n",
    "        writer.add_scalars(\n",
    "            'overall/nrmse', {\n",
    "                f'val/{ratio}/{contrast_1}' : avg_cost[contrast_1][epoch, 7],\n",
    "                f'val/{ratio}/{contrast_2}' : avg_cost[contrast_2][epoch, 7],\n",
    "            }, \n",
    "            epoch\n",
    "        )\n",
    "\n",
    "    writer.add_text(\n",
    "        'parameters', \n",
    "        f'{count_parameters(model)} parameters'\n",
    "    )\n",
    "    \n",
    "    ###opts###\n",
    "    writer.add_hparams(\n",
    "        _param_dict(0.001, 2, [6], [0], [0.06]), \n",
    "        {'overall/l1':0}\n",
    "    )\n",
    "    \n",
    "def write_tensorboard_one_contrasts(writer, avg_cost, model, total_epochs, ratio, opt):\n",
    "    #write to tensorboard ###opt###\n",
    "    contrast_1, _ = avg_cost.keys()\n",
    "    \n",
    "    for epoch in range(total_epochs):\n",
    "        \n",
    "        writer.add_scalars(\n",
    "            f'{ratio}/l1', {\n",
    "                f'train/{contrast_1}' : avg_cost[contrast_1][epoch, 0],\n",
    "                f'val/{contrast_1}' : avg_cost[contrast_1][epoch, 4],\n",
    "            }, \n",
    "            epoch\n",
    "        )\n",
    "\n",
    "        writer.add_scalars(\n",
    "            f'{ratio}/ssim', {\n",
    "                f'train/{contrast_1}' : avg_cost[contrast_1][epoch, 1],\n",
    "                f'val/{contrast_1}' : avg_cost[contrast_1][epoch, 5],\n",
    "            }, \n",
    "            epoch\n",
    "        )\n",
    "\n",
    "        writer.add_scalars(\n",
    "            f'{ratio}/psnr', {\n",
    "                f'train/{contrast_1}' : avg_cost[contrast_1][epoch, 2],\n",
    "                f'val/{contrast_1}' : avg_cost[contrast_1][epoch, 6],\n",
    "            }, \n",
    "            epoch\n",
    "        )\n",
    "\n",
    "        writer.add_scalars(\n",
    "            f'{ratio}/nrmse', {\n",
    "                f'train/{contrast_1}' : avg_cost[contrast_1][epoch, 3],\n",
    "                f'val/{contrast_1}' : avg_cost[contrast_1][epoch, 7],\n",
    "            }, \n",
    "            epoch\n",
    "        )\n",
    "\n",
    "    writer.add_text(\n",
    "        'parameters', \n",
    "        f'{count_parameters(model)} parameters'\n",
    "    )\n",
    "    \n",
    "    ###opts###\n",
    "    writer.add_hparams(\n",
    "        _param_dict(0.001, 2, [6], [0], [0.06]), \n",
    "        {f'{ratio}/l1':0}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2b4aada",
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion(im_fs: torch.Tensor, im_us: torch.Tensor):\n",
    "    '''\n",
    "    @parameter im_us: undersampled image (2D)\n",
    "    @parameter im_fs: fully sampled image (2D)\n",
    "    should be on GPU device for fast computation\n",
    "    '''\n",
    "    \n",
    "    # use l1 loss between two images\n",
    "    criterion = nn.L1Loss()\n",
    "    \n",
    "    # can add more fancy loss functions here later\n",
    "    \n",
    "    return criterion(im_us, im_fs)\n",
    "\n",
    "def metrics(im_fs: torch.Tensor, im_us: torch.Tensor):\n",
    "    '''\n",
    "    @parameter im_us: undersampled image (2D)\n",
    "    @parameter im_fs: fully sampled image (2D)\n",
    "    should be on GPU device for fast computation\n",
    "    '''\n",
    "\n",
    "    # change to ndarray\n",
    "    im_us = transforms.tensor_to_complex_np(im_us.cpu().detach())\n",
    "    im_fs = transforms.tensor_to_complex_np(im_fs.cpu().detach())\n",
    "    \n",
    "    # convert complex nums to magnitude\n",
    "    im_us = np.absolute(im_us)\n",
    "    im_fs = np.absolute(im_fs)\n",
    "    \n",
    "    im_us = im_us.reshape(\n",
    "        (im_us.shape[2], im_us.shape[3])\n",
    "    )\n",
    "    \n",
    "    im_fs = im_fs.reshape(\n",
    "        (im_fs.shape[2], im_fs.shape[3])\n",
    "    )\n",
    "    \n",
    "    # psnr\n",
    "    psnr = skimage.metrics.peak_signal_noise_ratio(\n",
    "        im_fs, \n",
    "        im_us, \n",
    "        data_range = np.max(im_fs) - np.min(im_fs)\n",
    "    )\n",
    "    \n",
    "    #nrmse\n",
    "    nrmse = skimage.metrics.normalized_root_mse(im_fs, im_us)\n",
    "    \n",
    "    # ssim\n",
    "    # normalize 0 to 1\n",
    "    im_fs -= np.min(im_fs)\n",
    "    im_fs /= np.max(im_fs)\n",
    "    im_us -= np.min(im_us)\n",
    "    im_us /= np.max(im_us)\n",
    "    \n",
    "    ssim = skimage.metrics.structural_similarity(im_fs, im_us, data_range = 1)\n",
    "    \n",
    "    return ssim, psnr, nrmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e67375d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "=========== Universal Single-task Trainer =========== \n",
    "code modified from https://github.com/lorenmt/mtan/blob/master/im2im_pred/utils.py\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def single_task_trainer(\n",
    "    train_loader, val_loader,\n",
    "    train_ratios, val_ratios,\n",
    "    single_task_model, device, writer, \n",
    "    optimizer, scheduler,\n",
    "    opt = 0, total_epochs=2 ###opt###\n",
    "):\n",
    "    \n",
    "    contrast_count = len(list(train_ratios.values()))\n",
    "    ratio = f\"N={'_N='.join(str(key) for key in train_ratios.values())}\"\n",
    "    \n",
    "    best_val_loss = np.infty\n",
    "    \n",
    "    # contains info for all epochs and contrasts\n",
    "    avg_cost = {\n",
    "        contrast : np.zeros([total_epochs, 8])\n",
    "        for contrast in train_ratios.keys()\n",
    "    }\n",
    "    avg_cost['overall'] = np.zeros([total_epochs, 8])\n",
    "    \n",
    "    for epoch in range(total_epochs):\n",
    "        # contains info for single batch of a single epoch\n",
    "        cost = np.zeros(8, dtype = np.float32)\n",
    "\n",
    "        # train the data\n",
    "        single_task_model.train()\n",
    "        train_batch = len(train_loader)\n",
    "        train_dataset = iter(train_loader)\n",
    "        \n",
    "        for kspace, mask, sens, im_fs, contrast in train_dataset:\n",
    "            contrast = contrast[0] # torch dataset loader returns as tuple\n",
    "            kspace, mask = kspace.to(device), mask.to(device)\n",
    "            sens, im_fs = sens.to(device), im_fs.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            _, im_us = single_task_model(kspace, mask, sens) # forward pass\n",
    "            loss = criterion(im_fs, im_us)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # losses and metrics are averaged over epoch\n",
    "            # L1 loss for now\n",
    "            cost[0] = loss.item() \n",
    "            # ssim, psnr, nrmse\n",
    "            cost[1], cost[2], cost[3] = metrics(im_fs, im_us)\n",
    "\n",
    "            # update overall\n",
    "            avg_cost[contrast][epoch, :4] += cost[:4] / train_ratios[contrast]\n",
    "            avg_cost['overall'][epoch, :4] += cost[:4] / train_batch\n",
    "\n",
    "        \n",
    "        # get losses and metrics for each epoch\n",
    "        single_task_model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_batch = len(val_loader)\n",
    "            \n",
    "#             # training data (calculate during training) start calculating loss / metrics after a few epochs\n",
    "#             train_dataset = iter(train_loader)\n",
    "#             for kspace, mask, sens, im_fs, contrast in train_dataset:\n",
    "#                 contrast = contrast[0]\n",
    "#                 kspace, mask = kspace.to(device), mask.to(device)\n",
    "#                 sens, im_fs = sens.to(device), im_fs.to(device)\n",
    "\n",
    "#                 _, im_us = single_task_model(kspace, mask, sens) # forward pass\n",
    "#                 loss = criterion(im_fs, im_us)\n",
    "                \n",
    "#                 # L1 loss for now\n",
    "#                 cost[0] = loss.item() \n",
    "#                 # ssim, psnr, nrmse\n",
    "#                 cost[1], cost[2], cost[3] = metrics(im_fs, im_us)\n",
    "\n",
    "#                 # update overall\n",
    "#                 avg_cost[contrast][epoch, :4] += cost[:4] / train_ratios[contrast]\n",
    "#                 avg_cost['overall'][epoch, :4] += cost[:4] / train_batch\n",
    "\n",
    "        \n",
    "            # validation data\n",
    "            val_dataset = iter(val_loader)\n",
    "            for val_idx, val_data in enumerate(val_dataset):\n",
    "                kspace, mask, sens, im_fs, contrast = val_data\n",
    "                contrast = contrast[0]\n",
    "                kspace, mask = kspace.to(device), mask.to(device)\n",
    "                sens, im_fs = sens.to(device), im_fs.to(device)\n",
    "\n",
    "                _, im_us = single_task_model(kspace, mask, sens) # forward pass\n",
    "                loss = criterion(im_fs, im_us)\n",
    "                \n",
    "                # L1 loss for now\n",
    "                cost[4] = loss.item()\n",
    "                # ssim, psnr, nrmse\n",
    "                cost[5], cost[6], cost[7] = metrics(im_fs, im_us)\n",
    "                \n",
    "                # update overall\n",
    "                avg_cost[contrast][epoch, 4:] += cost[4:] / val_ratios[contrast]\n",
    "                avg_cost['overall'][epoch, 4:] += cost[4:] / val_batch\n",
    "                \n",
    "               # visualize reconstruction every few epochs\n",
    "                if epoch % 1 == 0 and True: ###opt###\n",
    "                    # if single contrast, only visualize 17th slice\n",
    "                    if (\n",
    "                        val_idx == 17 or \n",
    "                        val_idx == val_batch - 17 and contrast_count > 1\n",
    "                    ):\n",
    "                        writer.add_figure(\n",
    "                            f'{ratio}/{contrast}', \n",
    "                            plot_quadrant(im_fs, im_us),\n",
    "                            epoch, close = True,\n",
    "                        )\n",
    "                               \n",
    "                \n",
    "        # early stopping        \n",
    "        if avg_cost['overall'][epoch, 4] < best_val_loss:\n",
    "            best_val_loss = avg_cost['overall'][epoch, 4]\n",
    "            torch.save(\n",
    "                single_task_model.state_dict(), \n",
    "                f'models/experiment_name/{ratio}_l1.pt'\n",
    "            ) ###opt###\n",
    "            \n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f'''\n",
    "        >Epoch: {epoch + 1:04d}\n",
    "        TRAIN: loss {avg_cost['overall'][epoch, 0]:.4f} | ssim {avg_cost['overall'][epoch, 1]:.4f} | psnr {avg_cost['overall'][epoch, 2]:.4f} | nrmse {avg_cost['overall'][epoch, 3]:.4f} \n",
    "        VAL: loss {avg_cost['overall'][epoch, 4]:.4f} | ssim {avg_cost['overall'][epoch, 5]:.4f} | psnr {avg_cost['overall'][epoch, 6]:.4f} | nrmse {avg_cost['overall'][epoch, 7]:.4f}\n",
    "        \n",
    "        ''')\n",
    "    \n",
    "    # write to tensorboard\n",
    "    ###opt###\n",
    "    if True:\n",
    "        write_tensorboard(writer, avg_cost, single_task_model, ratio, opt)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d34ad2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(scarcities, dataset_names):\n",
    "    basedirs = [\n",
    "        f'/mnt/dense/vliu/summer_dset/{dataset_name}'\n",
    "        for dataset_name in dataset_names\n",
    "    ]\n",
    "    \n",
    "    for scarcity in scarcities:\n",
    "        print(f'experiment w scarcity {scarcity}')\n",
    "        train_dloader = genDataLoader(\n",
    "            [f'{basedir}/Train' for basedir in basedirs], # choose randomly\n",
    "            [4, scarcity] # downsample\n",
    "        )\n",
    "\n",
    "        val_dloader = genDataLoader(\n",
    "            [f'{basedir}/Val' for basedir in basedirs], # choose randomly\n",
    "            [4, scarcity], # no downsampling\n",
    "            shuffle = False,\n",
    "        )\n",
    "        print('generated dataloaders')\n",
    "\n",
    "        # other inputs to STL wrapper\n",
    "        device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "        varnet = VarNet().to(device)\n",
    "\n",
    "        optimizer = torch.optim.Adam(varnet.parameters(),lr=0.001)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=200, gamma=0.5)\n",
    "        print('start training')\n",
    "        single_task_trainer(\n",
    "            train_dloader[0], val_dloader[0], \n",
    "            train_dloader[1], val_dloader[1], # ratios dicts\n",
    "            varnet, device, writer_tensorboard,\n",
    "            optimizer, scheduler,\n",
    "            opt = 0, total_epochs=2\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19a2d206",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('modelsa/hi'):\n",
    "    os.makedirs('modelsa/hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec6cb0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# two contrasts\n",
    "# datasets\n",
    "scarcities = [2, 4]\n",
    "\n",
    "dataset_names = [\n",
    "    'div_coronal_pd',\n",
    "    'div_coronal_pd_fs',\n",
    "]\n",
    "\n",
    "# # single contrast\n",
    "# # datasets\n",
    "# scarcities = [4]\n",
    "\n",
    "# dataset_names = [\n",
    "#     'div_coronal_pd',\n",
    "# ]\n",
    "\n",
    "run_name = f\"demo/STL_{'_'.join(dataset_names)}\"\n",
    "writer_tensorboard = SummaryWriter(log_dir = run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ef11cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment w scarcity 2\n",
      "generated dataloaders\n",
      "start training\n",
      "\n",
      "        >Epoch: 0001\n",
      "        TRAIN: loss 0.0522 | ssim 0.7002 | psnr 29.3284 | nrmse 0.2586 \n",
      "        VAL: loss 0.0418 | ssim 0.7517 | psnr 30.4592 | nrmse 0.2188\n",
      "        \n",
      "        \n",
      "\n",
      "        >Epoch: 0002\n",
      "        TRAIN: loss 0.0450 | ssim 0.7466 | psnr 30.9216 | nrmse 0.2141 \n",
      "        VAL: loss 0.0405 | ssim 0.7717 | psnr 30.9871 | nrmse 0.2072\n",
      "        \n",
      "        \n",
      "experiment w scarcity 4\n",
      "generated dataloaders\n",
      "start training\n",
      "\n",
      "        >Epoch: 0001\n",
      "        TRAIN: loss 0.0520 | ssim 0.6841 | psnr 28.3595 | nrmse 0.2706 \n",
      "        VAL: loss 0.0437 | ssim 0.7498 | psnr 30.1756 | nrmse 0.2248\n",
      "        \n",
      "        \n",
      "\n",
      "        >Epoch: 0002\n",
      "        TRAIN: loss 0.0413 | ssim 0.7566 | psnr 30.0946 | nrmse 0.2175 \n",
      "        VAL: loss 0.0423 | ssim 0.7435 | psnr 30.8858 | nrmse 0.2104\n",
      "        \n",
      "        \n"
     ]
    }
   ],
   "source": [
    "main(scarcities, dataset_names)\n",
    "writer_tensorboard.flush()\n",
    "writer_tensorboard.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70d18de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "filedir = f\"models/{opt.experimentname}_{opt.network}_{'_'.join(opt.datasets)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "545a5d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filedir = f\"models/STL_varnet_div_coronal_pd_div_coronal_pd_fs\"\n",
    "modelnames = os.listdir(model_filedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29381fd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['N=38_N=40_l1.pt', 'N=494_N=511_l1.pt', 'N=494_N=259_l1.pt']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "059bfbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = [\n",
    "    'div_coronal_pd',\n",
    "    'div_coronal_pd_fs',\n",
    "]\n",
    "\n",
    "basedirs = [\n",
    "    f'/mnt/dense/vliu/summer_dset/{dataset_name}'\n",
    "    for dataset_name in dataset_names\n",
    "]\n",
    "\n",
    "test_dloader = genDataLoader(\n",
    "    [f'{basedir}/Test' for basedir in basedirs], # choose randomly\n",
    "    [4, 4],\n",
    "    shuffle = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b19e726b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "316e02cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:3'\n",
    "with torch.no_grad():\n",
    "    the_model = VarNet().to(device)\n",
    "df_row = np.zeros([len(modelnames), 6])\n",
    "for idx, model in enumerate(modelnames):\n",
    "\n",
    "    \n",
    "    model_filepath = os.path.join(model_filedir, model)\n",
    "\n",
    "    # load model\n",
    "    the_model.load_state_dict(torch.load(model_filepath))\n",
    "    \n",
    "    # iterate thru test set\n",
    "    ####################### separate two contrasts in test set ############################\n",
    "    test_batch = len(test_dloader)\n",
    "    test_dataset = iter(test_dloader[0])\n",
    "    for test_idx, test_data in enumerate(test_dataset):\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            kspace, mask, sens, im_fs, contrast = test_data\n",
    "            contrast = contrast[0]\n",
    "            kspace, mask = kspace.to(device), mask.to(device)\n",
    "            sens, im_fs = sens.to(device), im_fs.to(device)\n",
    "\n",
    "            _, im_us = the_model(kspace, mask, sens) # forward pass\n",
    "            \n",
    "            # L1 loss for now\n",
    "            loss = criterion(im_fs, im_us)\n",
    "            df_row[idx][0] += loss.item() / test_batch\n",
    "            \n",
    "            # ssim, psnr, nrmse\n",
    "            ssim, psnr, nrmse = metrics(im_fs, im_us)\n",
    "            for j in range(3):\n",
    "                df_row[idx][j + 1] += metrics(im_fs, im_us)[j] / test_batch\n",
    "    \n",
    "    # define x axis\n",
    "    ratio_1 = int(model.split('_')[0].split('=')[1])\n",
    "    ratio_2 = int(model.split('_')[1].split('=')[1])\n",
    "    df_row[idx][4] = ratio_1\n",
    "    df_row[idx][5] = ratio_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c2cdef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1.45900832,   25.10502428, 1025.94458722,    7.53519702,\n",
       "          38.        ,   40.        ],\n",
       "       [   1.18314077,   27.24668827, 1147.36300301,    5.34349928,\n",
       "         494.        ,  511.        ],\n",
       "       [   1.20969392,   26.73536128, 1130.10879287,    5.61200281,\n",
       "         494.        ,  259.        ]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9838fdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "contrast1 = 'div coronal proton density'\n",
    "contrast2 = 'div coronal proton density fat suppression'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "db5e1ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(\n",
    "    df_row,\n",
    "    columns=['loss', 'ssim', 'psnr', 'nrmse', contrast1, contrast2]\n",
    ")\n",
    "df2 = df2.drop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4661cd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bokeh.plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1699dc04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>ssim</th>\n",
       "      <th>psnr</th>\n",
       "      <th>nrmse</th>\n",
       "      <th>div coronal proton density</th>\n",
       "      <th>div coronal proton density fat suppression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.183141</td>\n",
       "      <td>27.246688</td>\n",
       "      <td>1147.363003</td>\n",
       "      <td>5.343499</td>\n",
       "      <td>494.0</td>\n",
       "      <td>511.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.209694</td>\n",
       "      <td>26.735361</td>\n",
       "      <td>1130.108793</td>\n",
       "      <td>5.612003</td>\n",
       "      <td>494.0</td>\n",
       "      <td>259.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss       ssim         psnr     nrmse  div coronal proton density  \\\n",
       "1  1.183141  27.246688  1147.363003  5.343499                       494.0   \n",
       "2  1.209694  26.735361  1130.108793  5.612003                       494.0   \n",
       "\n",
       "   div coronal proton density fat suppression  \n",
       "1                                       511.0  \n",
       "2                                       259.0  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7f571ecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1703\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1703\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error(url) {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < css_urls.length; i++) {\n",
       "      const url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.3.2.min.js\": \"XypntL49z55iwGVUW4qsEu83zKL3XEcz0MjuGOQ9SlaaQ68X/g+k1FcioZi7oQAc\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.3.2.min.js\": \"bEsM86IHGDTLCS0Zod8a8WM6Y4+lafAL/eSiyQcuPzinmWNgNO2/olUF0Z2Dkn5i\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.3.2.min.js\": \"TX0gSQTdXTTeScqxj6PVQxTiRW8DOoGVwinyi1D3kxv7wuxQ02XkOxv0xwiypcAH\"};\n",
       "\n",
       "    for (let i = 0; i < js_urls.length; i++) {\n",
       "      const url = js_urls[i];\n",
       "      const element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      if (url in hashes) {\n",
       "        element.crossOrigin = \"anonymous\";\n",
       "        element.integrity = \"sha384-\" + hashes[url];\n",
       "      }\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  \n",
       "  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.3.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.3.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.3.2.min.js\"];\n",
       "  var css_urls = [];\n",
       "  \n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "    \n",
       "    \n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      \n",
       "    for (var i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "    if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1703\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1703\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.3.2.min.js\": \"XypntL49z55iwGVUW4qsEu83zKL3XEcz0MjuGOQ9SlaaQ68X/g+k1FcioZi7oQAc\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.3.2.min.js\": \"bEsM86IHGDTLCS0Zod8a8WM6Y4+lafAL/eSiyQcuPzinmWNgNO2/olUF0Z2Dkn5i\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.3.2.min.js\": \"TX0gSQTdXTTeScqxj6PVQxTiRW8DOoGVwinyi1D3kxv7wuxQ02XkOxv0xwiypcAH\"};\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      if (url in hashes) {\n        element.crossOrigin = \"anonymous\";\n        element.integrity = \"sha384-\" + hashes[url];\n      }\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.3.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.3.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.3.2.min.js\"];\n  var css_urls = [];\n  \n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (var i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1703\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "  <div class=\"bk-root\" id=\"71947e68-ce8f-4312-be04-db5377a3899a\" data-root-id=\"1704\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "    \n",
       "  var docs_json = {\"91ae7c29-c20c-427a-9594-208d1733c206\":{\"defs\":[],\"roots\":{\"references\":[{\"attributes\":{\"below\":[{\"id\":\"1713\"}],\"center\":[{\"id\":\"1716\"},{\"id\":\"1720\"},{\"id\":\"1754\"}],\"height\":400,\"left\":[{\"id\":\"1717\"}],\"renderers\":[{\"id\":\"1741\"},{\"id\":\"1760\"}],\"title\":\"loss\",\"toolbar\":{\"id\":\"1729\"},\"x_range\":{\"id\":\"1705\"},\"x_scale\":{\"id\":\"1709\"},\"y_range\":{\"id\":\"1707\"},\"y_scale\":{\"id\":\"1711\"}},\"id\":\"1704\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"axis_label\":\"div coronal proton density fat suppression\",\"formatter\":{\"id\":\"1750\"},\"major_label_policy\":{\"id\":\"1748\"},\"ticker\":{\"id\":\"1714\"}},\"id\":\"1713\",\"type\":\"LinearAxis\"},{\"attributes\":{},\"id\":\"1745\",\"type\":\"AllLabels\"},{\"attributes\":{},\"id\":\"1750\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"data\":{\"div coronal proton density\":{\"__ndarray__\":\"AAAAAADgfkAAAAAAAOB+QA==\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[2]},\"div coronal proton density fat suppression\":{\"__ndarray__\":\"AAAAAADwf0AAAAAAADBwQA==\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[2]},\"index\":[1,2],\"loss\":{\"__ndarray__\":\"AADABCXu8j8AAAAE6FrzPw==\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[2]},\"nrmse\":{\"__ndarray__\":\"bLBZRr5fFUA+yg/dsHIWQA==\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[2]},\"psnr\":{\"__ndarray__\":\"wJsPt3PtkUBczWVnb6iRQA==\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[2]},\"ssim\":{\"__ndarray__\":\"Zapd9iY/O0CUJ/iiQLw6QA==\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[2]}},\"selected\":{\"id\":\"1751\"},\"selection_policy\":{\"id\":\"1752\"}},\"id\":\"1737\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":0.5,\"fill_color\":\"lightgrey\",\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":1.0,\"line_color\":\"black\",\"line_dash\":[4,4],\"line_width\":2,\"right_units\":\"screen\",\"syncable\":false,\"top_units\":\"screen\"},\"id\":\"1727\",\"type\":\"BoxAnnotation\"},{\"attributes\":{\"label\":{\"value\":\"first contrast\"},\"renderers\":[{\"id\":\"1741\"},{\"id\":\"1760\"}]},\"id\":\"1755\",\"type\":\"LegendItem\"},{\"attributes\":{},\"id\":\"1705\",\"type\":\"DataRange1d\"},{\"attributes\":{\"data\":{\"div coronal proton density\":{\"__ndarray__\":\"AAAAAADgfkAAAAAAAOB+QA==\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[2]},\"div coronal proton density fat suppression\":{\"__ndarray__\":\"AAAAAADwf0AAAAAAADBwQA==\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[2]},\"index\":[1,2],\"loss\":{\"__ndarray__\":\"AADABCXu8j8AAAAE6FrzPw==\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[2]},\"nrmse\":{\"__ndarray__\":\"bLBZRr5fFUA+yg/dsHIWQA==\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[2]},\"psnr\":{\"__ndarray__\":\"wJsPt3PtkUBczWVnb6iRQA==\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[2]},\"ssim\":{\"__ndarray__\":\"Zapd9iY/O0CUJ/iiQLw6QA==\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[2]}},\"selected\":{\"id\":\"1772\"},\"selection_policy\":{\"id\":\"1773\"}},\"id\":\"1756\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"1772\",\"type\":\"Selection\"},{\"attributes\":{},\"id\":\"1751\",\"type\":\"Selection\"},{\"attributes\":{},\"id\":\"1752\",\"type\":\"UnionRenderers\"},{\"attributes\":{},\"id\":\"1722\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"click_policy\":\"hide\",\"items\":[{\"id\":\"1755\"}]},\"id\":\"1754\",\"type\":\"Legend\"},{\"attributes\":{},\"id\":\"1714\",\"type\":\"BasicTicker\"},{\"attributes\":{\"line_color\":\"#1f77b4\",\"x\":{\"field\":\"div coronal proton density fat suppression\"},\"y\":{\"field\":\"loss\"}},\"id\":\"1758\",\"type\":\"Line\"},{\"attributes\":{\"data_source\":{\"id\":\"1756\"},\"glyph\":{\"id\":\"1758\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"1759\"},\"view\":{\"id\":\"1761\"}},\"id\":\"1760\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"overlay\":{\"id\":\"1727\"}},\"id\":\"1723\",\"type\":\"BoxZoomTool\"},{\"attributes\":{\"source\":{\"id\":\"1756\"}},\"id\":\"1761\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"1718\",\"type\":\"BasicTicker\"},{\"attributes\":{\"axis\":{\"id\":\"1713\"},\"ticker\":null},\"id\":\"1716\",\"type\":\"Grid\"},{\"attributes\":{\"source\":{\"id\":\"1737\"}},\"id\":\"1742\",\"type\":\"CDSView\"},{\"attributes\":{\"line_alpha\":0.1,\"line_color\":\"#1f77b4\",\"x\":{\"field\":\"div coronal proton density fat suppression\"},\"y\":{\"field\":\"loss\"}},\"id\":\"1759\",\"type\":\"Line\"},{\"attributes\":{},\"id\":\"1709\",\"type\":\"LinearScale\"},{\"attributes\":{},\"id\":\"1707\",\"type\":\"DataRange1d\"},{\"attributes\":{\"axis_label\":\"loss\",\"formatter\":{\"id\":\"1747\"},\"major_label_policy\":{\"id\":\"1745\"},\"ticker\":{\"id\":\"1718\"}},\"id\":\"1717\",\"type\":\"LinearAxis\"},{\"attributes\":{},\"id\":\"1773\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"data_source\":{\"id\":\"1737\"},\"glyph\":{\"id\":\"1739\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"1740\"},\"view\":{\"id\":\"1742\"}},\"id\":\"1741\",\"type\":\"GlyphRenderer\"},{\"attributes\":{},\"id\":\"1725\",\"type\":\"ResetTool\"},{\"attributes\":{},\"id\":\"1748\",\"type\":\"AllLabels\"},{\"attributes\":{},\"id\":\"1726\",\"type\":\"HelpTool\"},{\"attributes\":{\"callback\":null,\"tooltips\":[[\"loss\",\"@{loss}\"],[\"contrast\",\"@div coronal proton density fat suppression\"]]},\"id\":\"1728\",\"type\":\"HoverTool\"},{\"attributes\":{\"active_multi\":null,\"tools\":[{\"id\":\"1721\"},{\"id\":\"1722\"},{\"id\":\"1723\"},{\"id\":\"1724\"},{\"id\":\"1725\"},{\"id\":\"1726\"},{\"id\":\"1728\"}]},\"id\":\"1729\",\"type\":\"Toolbar\"},{\"attributes\":{},\"id\":\"1721\",\"type\":\"PanTool\"},{\"attributes\":{\"axis\":{\"id\":\"1717\"},\"dimension\":1,\"ticker\":null},\"id\":\"1720\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"1747\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{},\"id\":\"1711\",\"type\":\"LinearScale\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"x\":{\"field\":\"div coronal proton density fat suppression\"},\"y\":{\"field\":\"loss\"}},\"id\":\"1740\",\"type\":\"Circle\"},{\"attributes\":{},\"id\":\"1724\",\"type\":\"SaveTool\"},{\"attributes\":{\"fill_color\":{\"value\":\"#1f77b4\"},\"line_color\":{\"value\":\"#1f77b4\"},\"x\":{\"field\":\"div coronal proton density fat suppression\"},\"y\":{\"field\":\"loss\"}},\"id\":\"1739\",\"type\":\"Circle\"}],\"root_ids\":[\"1704\"]},\"title\":\"Bokeh Application\",\"version\":\"2.3.2\"}};\n",
       "  var render_items = [{\"docid\":\"91ae7c29-c20c-427a-9594-208d1733c206\",\"root_ids\":[\"1704\"],\"roots\":{\"1704\":\"71947e68-ce8f-4312-be04-db5377a3899a\"}}];\n",
       "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else {\n",
       "        attempts++;\n",
       "        if (attempts > 100) {\n",
       "          clearInterval(timer);\n",
       "          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        }\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "1704"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "bokeh.io.output_notebook()\n",
    "# For convenience\n",
    "x = contrast2\n",
    "y = \"loss\"\n",
    "\n",
    "# Make figure\n",
    "p = bokeh.plotting.figure(\n",
    "    width=600,\n",
    "    height=400,\n",
    "    x_axis_label=x,\n",
    "    y_axis_label=y,\n",
    "    tooltips=[\n",
    "        (\"loss\", \"@{loss}\"),\n",
    "        (\"contrast\", f\"@{contrast2}\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Add glyphs\n",
    "p.circle(\n",
    "    source=df2,\n",
    "    x=x,\n",
    "    y=y,\n",
    "    legend_label=\"first contrast\",\n",
    ")\n",
    "\n",
    "p.line(\n",
    "    source=df2,\n",
    "    x=x,\n",
    "    y=y,\n",
    "    legend_label=\"first contrast\",\n",
    ")\n",
    "\n",
    "p.legend.location = \"top_right\"\n",
    "p.legend.click_policy = \"hide\"\n",
    "p.title = \"loss\"\n",
    "\n",
    "bokeh.io.show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2109321a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
